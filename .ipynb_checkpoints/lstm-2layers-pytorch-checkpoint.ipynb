{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "if torch.cuda.is_available(): device = 'cuda'\n",
    "else: device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "filename = 'Tuyen-Tap-To-Hoai-To-Hoai.txt'\n",
    "with open(filename, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "# encode the text and map each character to an integer and vice versa\n",
    "\n",
    "# we create two dictionaries:\n",
    "# 1. int2char, which maps integers to characters\n",
    "# 2. char2int, which maps characters to unique integers\n",
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "# encoded = np.array([ch for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([131,  45, 116, 110,  50,  45, 116, 112, 112, 112,  93,  27, 118,\n",
       "       121, 202, 200, 110,  37, 130, 112, 112,  93,  27, 118, 121, 202,\n",
       "       200, 110, 195, 130, 112, 112,  93,  27, 118, 121, 202, 200, 110,\n",
       "        48, 130, 112, 112,  93,  27, 118, 121, 202, 200, 110, 145, 130,\n",
       "       112, 112,  93,  27, 118, 121, 202, 200, 110, 160, 130, 112, 112,\n",
       "        93,  27, 118, 121, 202, 200, 110, 113, 130, 112, 112,  93,  27,\n",
       "       118, 121, 202, 200, 110, 156, 130, 112, 112, 112, 112, 112, 112,\n",
       "       147, 117, 110,  69, 200, 118, 154, 201, 110])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot\n",
    "\n",
    "def save_checkpoint(file_name):\n",
    "    model_dante = file_name+'.net'\n",
    "\n",
    "    checkpoint = {'n_hidden': net.n_hidden,\n",
    "                  'n_layers': net.n_layers,\n",
    "                  'state_dict': net.state_dict(),\n",
    "                  'tokens': net.chars}\n",
    "\n",
    "    with open(model_dante, 'wb') as f:\n",
    "        torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    # start at random location within seg_length\n",
    "    start = int(np.random.choice(range(seq_length),1))\n",
    "    arr = arr[start:]\n",
    "    \n",
    "    # total number of batches we can make, // integer division, round down\n",
    "    batch_size_total = batch_size * seq_length\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    \n",
    "    # Reshape into batch_size rows, n. of first row is the batch size, the other lenght is inferred\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        # TODO: investigate\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y \n",
    "        \n",
    "#when we call get batches we are going \n",
    "#to create a generator that iteratest through our array and returns x, y with yield command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=612, n_layers=4,\n",
    "                               drop_prob=0.5, lr=0.001, bidirectional=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        if bidirectional: self.D = 2\n",
    "        else: self.D = 1\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        # define encoder\n",
    "        self.embedding = nn.Embedding(len(self.chars), n_hidden)\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "#         self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "#                             dropout=drop_prob, batch_first=True)\n",
    "        self.lstm = nn.LSTM(n_hidden, n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True, bidirectional=bidirectional)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(self.D*n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "        \n",
    "        word_embedded = self.embedding(x)        \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "#         r_output, hidden = self.lstm(x, hidden)\n",
    "        r_output, hidden = self.lstm(word_embedded, hidden)\n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.D*self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "       \n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "       \n",
    "        # TODO: investigate\n",
    "        hidden = (weight.new(self.D*self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n",
    "              weight.new(self.D*self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "#         x = one_hot_encode(x, len(net.chars))\n",
    "\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        # apply softmax to get p probabilities for the likely next character giving x\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        # considering the k most probable characters with topk method\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='Cháu lên ba ', top_k=None):\n",
    "        \n",
    "\n",
    "\n",
    "    net.eval() # eval mode\n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "    net.train() # train mode\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10, checkpoint='data.net'):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train() # set training mode for dropout and batchnorm\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    h = net.init_hidden(batch_size)\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        \n",
    "        counter = 0\n",
    "        step = len(data)//(batch_size*seq_length)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "#             x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h]) # similar to detach\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "#             if counter % print_every == 0:\n",
    "            if counter % step == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = 0\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "#                     x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                    val_losses = 0.9*val_losses +0.1*val_loss.item()\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}/{}...\".format(counter,step),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(val_losses))\n",
    "        print(sample(net, 1000, prime='Thế nào mà anh ấy bị lây cái máu dê của thằng địa đến tận giờ.', top_k=5))\n",
    "        save_checkpoint(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (embedding): Embedding(204, 512)\n",
      "  (lstm): LSTM(512, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=204, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_length = 200 #max length verses\n",
    "n_epochs = 50 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "# train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=1, checkpoint=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nhập 1 câu bất kỳ: cháu lên ba\n",
      "\n",
      "cháu lên ba mươi. Nhà tôi nói như tiếng gà giáo nó thì đi tìm đâu. Một bọn chim chít khóc nóng đến tận thế. Cái này ta đi tìm cả trưởng thôn, có chịu không biết như chúng mày còn nói đi với con chim đi chợ. Nhưng chúng tôi trông tin nằm đi thế này, tôi đã được nói với chị.\n",
      "\n",
      "Chúng nó đi tìm chúng tôi thì đi chợ đã trở dậy. Một con trâu nhìn thấy một con gấu đã lại nghĩ ngợi như ngày trước còn nhớ chuyện những nơi chỉ còn có một nơi thì đã chạy về.\n",
      "\n",
      "Mỗi lúc nhớ các cụ thì thào cho các bạn nhà thìa chiếc chiếc lá. Con người đã chạy đi chợ thì đi đâu. Trời đã lâu. Chỉ nghe một lúc thì người ta đi đến nghe tiếng chuông chuyện trong thành các người chết được. Trên thuyền thuê, chỉ được một câu nói cho chú con nó như thế. Nghe, chúng nó lại chết đi, nghĩ ngượng nghiêng, thế là cái giống thiết được nghĩ những chuyện đi, nhưng đi tìm được những điều nhiều chuyện cũng nghĩ thế mà chỉ thấy cái gì không biết trông thấy một người thày giáo chẳng cần có cái giống của các, mà chỉ nhớ cái thằng chim chim vẫn đi đánh thế, đã cho con cái đương ngồi chạy. Một người nghe tiếng gà chim. Trên ca to, cái túi nhỏ như một người chết nghiêng. Nó lấy thế mà chỉ nghĩ thế là người ta chưa có. Tôi chạy về, thì chỉ thấy tôi chẳng cần chúng tôi nghe tiếng người đã đến ngoài bờ biển.\n",
      "\n",
      "Chúng tôi chưa đủ thuộc. Được mặt con chó, cái thúng, một cách đương đi theo. Tôi đương chạy cho tôi được nghĩ, tiếng gió ngược ra. Người đàn bà như thế nào cũng đương ngồi như những chiếc trống chuyện. Ngày trước mới nghĩ đi tìm mất thì có những người nhớ những đài chít khăn khăn trong người đi tìm chỗ nào. Thế là chú thiên hạ, có cái tiếng trong cái cột trang của một tay ngoài chuồng cho tôi.\n",
      "\n",
      "Những câu trong chân của cái cầu chuyện đã làm đi vào chợ có thể chỉ nhận ra, đã là một cái cổ, chẳng còn người tan chết. Có thể đi đâu chẳng biết đã được đi chơi đi thì cũng là miếng đấu đã, đi trong ngõ, chúng ta đã chạy ra chợ nào đây. Tôi đã nghĩ ra có người cứng thế, chẳng biết thế nào, ngày thì tôi đã đi được thế nào, tôi không biết cái truyện chỉ còn thế.\n",
      "\n",
      "- Ta cũng nhớ tôi.\n",
      "\n",
      "- Chẳng được cả nhà. Được có thế mà cũng không thể để thế thì ngày trước đến thằng chánh đi cho trước. Có lúc nó trở về đây chẳng cho đi tìm. Nhưng chúng tôi đã cho chết.\n",
      "\n",
      "Chúng tôi chưa có cánh gì, nhưng chúng nó là mình đã có cái nhà nước. Có người nhìn cái gì thế, tôi làm có người làm. Thế là nó tưởng cái tiền thì ngồi đi thì chết, chỉ nghĩ cách nhà thằng con gái thế này là thằng này. Nhưng tôi còn nhớ cho tôi, thế mà cái nó thì cũng đã có chúng nó đi tìm đi chuyện đi chợ đã tối được thì đi đâu, thằng cháu thì đã có người đi tìm ta đi chợ cho nó.\n",
      "\n",
      "Người đi đâu, nhưng như nghe có người thì chỉ nghe mà tinh chết, chưa biết đâu còn thấy cái thằng thằng chó đã được đi thì được cái nào, nó được nhớn nhác ngồi chạy thì. Mỗi lần tôi chưa đi trong cái tiền có cho nhớ, thầy thằng trong những câu tôi chưa được chuyển theo, nhưng nói thế, tôi cũng đương trông cho con trẻ chết thì đến đây đi đâu, đã thấy, chẳng còn thấy. Người ta có cái thằng con gái thì cũng có người đi chợ chiếc cho tôi nhìn.\n",
      "\n",
      "Chiếc bánh đúc ngoài chợ chẳng buồn như người đương chạy về chơi. Thế nào, đã làm được cái gì mà thì trong làng. Trông một cái đầu nghiện cho đã lên đến tận chợ thì thấy người ta tưởng nhưng đã lại thất thểu được một cách thế nào, tôi không cần thấy chui trong thung thuốc đã được thành những thế nào. Cũng không cần thấy mà đi tìm cách này chẳng biết nghĩ nói thêm thế. Cái nhà tôi cũng nghĩ những chuyện cho nghĩa đi nghiêm thiệp nhưng như trước nhất. Đến lần đã trông thấy thầy giáo thì cũng đã đến nhà chúng tôi.\n",
      "\n",
      "- Chú đã được thế nào được cho đi, tôi chỉ nhớ nhà có nó đi, tôi chỉ nghe tôi đương nói tiếng gáy trên đầu cho chúng con.\n",
      "\n",
      "- Chẳng thể có tiền, thằng này chỉ còn có cái tin lắm như thế mà. Đương đi có tay đã từng nhất!\n",
      "\n",
      "Tôi nghe tiếng trẻ trẻ thế này đã có người đi vào đây. Những ngày còn thường có cây chân tanh và người đi thu đi. Trên cây tre, cái thuyền đi và đi. Mon lại còn cho ta chịu khó nhìn nó như trôi thì nhưng những tiếng ngồi ngoảnh mặt lên. Có lần như trông thấy chú không có chân cái chân, đánh trong bãi nước. Cái gì cũng đã đánh ngay. Chúc lãnh Quang nhìn ra những cái gì này.\n",
      "\n",
      "Ngày mùa này có trong người còn nói chuyện đã thành mình cũng nhưng có người trông rõ nó cũng không chết được như thấy.\n",
      "\n",
      "- Cho đi, cho nó có.\n",
      "\n",
      "Những câu tôi nằm thẳng trong nước đương đi, nhưng cũng đến nhìn lại, những cây ngoài chợ của thì trong chiếm người ta trở lại, chỉ được thấy chiều chơi trong cái chợ của chị em chúng tôi còn có thể đi đâu mà đi tìm chỗ nào cũng có một mình.\n",
      "\n",
      "Chúng tôi thì trở về, có khi thế, đương thì đi trên thành cửa đình, nếu có lần chú điểm của tôi. Cũng nghe thấy tôi đi tìm.\n",
      "\n",
      "- Cái ngước cái giống chó đã cho nên tôi là chúng nó.\n",
      "\n",
      "Cái nhà trời này đã làm cả chục lần thôi. Chúng tôi đã thuộc được các bạn người ta đã thấy chúng ta cũng chỉ có người đi tìm chúng tôi đi, chưa đi thì chỉ được thế là chỉ nhập lại đi được như thế, trong khi chúng tôi chưa thấy nhau thì có lúc chỉ có mình có lẽ chỉ còn có chuyện. Cái chiếu cương nhanh như thế này thế này đã làm thế nào nữa, tao đi nhiều người đến thì đến trường chẳng biết chúng tôi đã đi trong truyện như cái tên những câu thu thiết thế, nhà nghiêm cũng có người.\n",
      "\n",
      "Những năm nay có công tác chiến dịch này. Nguyễn Bính cũng có nghĩa là đi đâu. Người đi chợ nhà, nhiều người ngồi thì còn đưa cho tôi đi được thế nào được. Người đi chơi thì được còn nhớ thì nàng chỉ thấy đi cách nhà trường của người ta đi theo được các cụ thiêu hoạch thế. Chỉ thấy chỉ có một lần ngoài cầu chỉ có các bộ thu chiếc thúng chân trắng như người trên tàu thiết trong cái điện đi đâu, trông lại đương trông thấy chỗ chiến trường.\n",
      "\n",
      "Những trận mới có cái gì trong cái tiền của ta có cách có nghe tôi đã thấy đến cái trận đã đến được trông cái chiếu chuyện có thể đi tìm được cho con trẻ.\n",
      "\n",
      "Câu nói nhảy choe vao. Nhưng tôi chẳng biết có thể chịu đứng đâu chết đâu. Nóng nhấp nghiêng, chỉ đến trên mặt.\n",
      "\n",
      "Trắt làm cách mạng cũng còn cho con chim tiếng chim trong trạng sách thì những người tron nhiệm thế nào. Chẳng chịu khó chịu còn trải một cái, người đi trong làng. Tôi thấy chúng nó chạy ra, chú nghiêm nhìn có nhiều, nhưng một lúc thấy chiều nghiên ngan. Những người thì tôi đã có nơi chiến tranh. Những ngày của người trong nhà đã tham quan, chỉ thấy người ta thì nghe chuyện đã trông thấy.\n",
      "\n",
      "Nhưng từ lúc trước một trận mưa. Các cụ nhà vua đi đường ngoài trường có cái nhanh, ngoài cổng trước cửa chơi nhà trường đã cắt chung vàng như nhà mình. Các ngõ chợ ngoài đồng đã được trả lời đã đi đường. Trên những thằng cái chuồng chuột cứ trông thấy trong làng. Chúng tôi cũng chẳng thấy, đến ngày trước thôi.\n",
      "\n",
      "Có con cho tôi nhớ cho con nó nói thế thì người ta có cho người có cái gì cũng đi. Chẳng thấy tin thưởng thế là đến cho được những trận chim vàng thì, chẳng còn nhớ, nó đã choảng trẻ, cũng có chuyện của một tháng. Có thể đi tìm cả nhà này.\n",
      "\n",
      "Cái nghiêu chuông người đi chơi chỉ nhìn trong cửa, đi đây thế. Nhưng nhưng như thế, chỉ thấy con gái chẳng có thì như nhiều người, những con mèo cũng như con chó thì chết, thằng người đi trong làng, trông thấy những con gái trên đầu con người còn có con người. Chúng tôi đi đây được. Các bác có con trai tôi đã có thể đến ngay. Cái chân thì chết có cái nón, chỉ được một người, người đã cắm trẻ con cũng đi thì cũng chẳng bao giờ tay được đến cả chuyện độc người ta. Chúng tôi làm ngược thì chẳng biết có thế, cũng chẳng thể có đi thuê chuyện thế nào. Tôi nhớ chú không có chiến tranh, tôi đi cũng chẳng trông thấy tôi thì cũng không biết đâu.\n",
      "\n",
      "Chỉ có cái gì từng chữ.\n",
      "\n",
      "Tôi đương thì tôi cũng không có nói thế. Đến ngày tôi đã cho đến cả đội thì tôi đi chợ đôi chút.\n",
      "\n",
      "Những cái điều như chẳng buồn thấy đã lây đến. Từ lúc trông thấy thì người ta đã thấy cái thuyền thưa như chúc thì cũng có lẽ như ngày ấy. Ngày ngày đi đường thì chẳng có nhà máy đã được có cái chiếc đá đen.\n",
      "\n",
      "Chiều chúng tôi thấy chẳng bao lâu trên cửa sổ, tiếng người thì tôi đi được đương thường chỉ để ý tin tứ chung chuyện.\n",
      "\n",
      "- Tôi là cái nhà này.\n",
      "\n",
      "- Tao chết, tao nhận lắm nào cũng chết. Tôi ngoảnh cái, tay đã có thể nghĩ thế nào thì nó, nhưng tôi cũng lại có chúng chẳng biết.\n",
      "\n",
      "- Chúng tôi thì thà thì đi thuyền đi vào.\n",
      "\n",
      "- Đi đường chết chứ. Chẳng có cái tay chúng mày chết, nhưng tôi đi chợ nhà chúng tôi. Thầy đi chợ thì đã thấy được một thằng chết, người nghe thằng đi đấy có. Thằng chó, thằng này nào đã bỏ thằng bé chết chết.\n",
      "\n",
      "- Các ông này chỉ có thằng thì đi đâu.\n",
      "\n",
      "- Cán bộ làm gì chứ đâu cũng được.\n",
      "\n",
      "Tôi có thể trở lại.\n",
      "\n",
      "- Các bạn cũng chỉ có thể nhớ thế nào?\n",
      "\n",
      "- Nhớn mắt. Có nghe đâu mà có tin đâu chẳng đi đâu. Nhà chúng nó lại đến như thế thật. Ngày trước, thằng này chưa chết.\n",
      "\n",
      "Ngày trước, đến chiều thì thành trước người nhìn thấy cái gì chưa thế. Có khi chỉ trông như con chuột chạy đi đến nhà các bạn thì thành nghe tiếng động như con gà chạy, thấy thằng chó đã còn nhớ cái thằng đội Quang có được chuyện thần thần đã chạy đến chỗ chiếc thúng tiết, cái nói: “Thế là như nó trông thấy, thế là điềm thì đã cho thì đi được thế này, thế nào cũng được có người đi đâu trên chợ, người ta chưa có thì đã trốn được thế.\n",
      "\n",
      "Nó cũng chẳng thể nói được. Chúng tôi thấy chỉ nhận đến nhà cán bộ này chỉ được chết được trước. Chúng tôi đi đây, có thể nghĩ thế. Chúng tôi còn nghĩ nhìn ngày mùa, chẳng được thuộc. Người ta thương có cái thằng nó thấy cho đến lúc nhìn trước mặt con trăn, con ngan thế chứ tôi cũng nghĩ thế mà chẳng biết chị đã cho tôi chết như thường đấy.\n",
      "\n",
      "Chú chim giảng và cũng có người chạy theo. Có người nói : “Tôi nghĩ ngợi có chiếc cái thúng cái đầu, đánh nhau nữa mà thôi.\n",
      "\n",
      "- Đi đâu, chú muốn đi.\n",
      "\n",
      "- Cái gì?\n",
      "\n",
      "- Đi đâu, tôi là tôi đi nói thế nào, nhưng chúng cũng như thế. Thế nào mà đi đâu, cũng không thể thấy thằng bé nhà tôi. Nhưng thằng trai lão đương nghe thấy có mấy con trâu chạy chạy về ngoài cửa chợ.\n",
      "\n",
      "Nhưng tôi đã nhiều người tan chân, chỉ người đi làm cho tôi thế. Người ta còn ngồi đi. Nghe nói tiếng thầy, có những đứa đi đâu thì được ngồi đầu tiên chỉ có thằng bay theo nhau như thế. Nghĩ ngợi chuyện đi thế nào, ch\n"
     ]
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open(filename+'.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "net = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "net.to(device)\n",
    "net.eval() # eval mode\n",
    "usr_input = input('Nhập 1 câu bất kỳ: ')\n",
    "print('\\n'+sample(net, 2000, top_k=4, prime=usr_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
