{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "import sys\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('bai_van_mau.txt','r',encoding='utf-8').read()\n",
    "data = data.lower()\n",
    "chars = list(set(data))\n",
    "\n",
    "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "datasize, vocab_size =  len(data),  len(chars)\n",
    "\n",
    "hidden_size = 128\n",
    "sequence_len = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(text, stride = 3):\n",
    "    \"\"\"\n",
    "    Create a training set by scanning a window of size sequence_len over the text corpus, with stride 3.\n",
    "    \"\"\"\n",
    "    text = data\n",
    "    Tx = sequence_len\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(0, len(text) - Tx, stride):\n",
    "        X.append(text[i: i + Tx])\n",
    "        Y.append(text[i + Tx])\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "\n",
    "X, Y = build_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 137, 10]),\n",
       " torch.Size([10]),\n",
       " 'sequence_len, vocab_size, batch_size')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vectorization(X, Y):\n",
    "    \"\"\"\n",
    "    Convert X and Y (lists or chars) into pytorch tensor to be given to a recurrent neural network.\n",
    "    \"\"\"\n",
    "    m = len(X)\n",
    "    \n",
    "\n",
    "    x = np.zeros((sequence_len, vocab_size, m))\n",
    "    y = np.zeros((m))\n",
    "    for i, sentence in enumerate(X):\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[t, char_to_ix[char], i] = 1\n",
    "        y[i] = char_to_ix[Y[i]]\n",
    "        \n",
    "    x = torch.Tensor(x).to(device)\n",
    "    y = torch.LongTensor(y).to(device)\n",
    "    return x, y\n",
    "\n",
    "# test vectorization\n",
    "x,y = vectorization(X[:10],Y[:10])\n",
    "x.shape,y.shape, 'sequence_len, vocab_size, batch_size'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), True, True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def initialize_lstm_parameters():\n",
    "    \"\"\"\n",
    "    Initialize parameters with small random values\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing:\n",
    "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
    "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
    "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
    "                        b --  Bias, numpy array of shape (n_a, 1)\n",
    "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    Wf = torch.randn(hidden_size, vocab_size + hidden_size, device=device)*0.01 \n",
    "    bf = torch.zeros((hidden_size, 1), requires_grad=True, device=device)\n",
    "    Wu = torch.randn(hidden_size, vocab_size + hidden_size, device=device)*0.01 \n",
    "    bu = torch.zeros((hidden_size, 1), requires_grad=True, device=device)\n",
    "    Wcc = torch.randn(hidden_size, vocab_size + hidden_size, device=device)*0.01 \n",
    "    bcc = torch.zeros((hidden_size, 1), requires_grad=True, device=device)\n",
    "    Wo = torch.randn(hidden_size, vocab_size + hidden_size, device=device)*0.01\n",
    "    bo = torch.zeros((hidden_size, 1), requires_grad=True, device=device)\n",
    "    Wy = torch.randn(vocab_size, hidden_size, device=device)*0.01\n",
    "    by = torch.zeros((vocab_size, 1), requires_grad=True, device=device)\n",
    "    \n",
    "    Wf.requires_grad = True\n",
    "    Wu.requires_grad = True\n",
    "    Wcc.requires_grad = True\n",
    "    Wo.requires_grad = True\n",
    "    Wy.requires_grad = True\n",
    "    \n",
    "    \n",
    "    parameters = [Wf, bf, Wu, bu, Wcc, bcc, Wo, bo, Wy, by]\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# test init paramter\n",
    "parameters = initialize_lstm_parameters()\n",
    "parameters[1].device, parameters[1].is_leaf, parameters[1].requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 100]), torch.Size([128, 100]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lstm_step_forward(x, a_prev, c_prev, parameters):\n",
    "    Wf, bf, Wu, bu, Wcc, bcc, Wo, bo, Wy, by = parameters\n",
    "  \n",
    "    concat = torch.cat((a_prev, x),axis=0)\n",
    "\n",
    "    f = torch.sigmoid(Wf @ concat + bf)\n",
    "    u = torch.sigmoid(Wu @ concat + bu)\n",
    "    cc = torch.tanh(Wcc @ concat + bcc)\n",
    "    o = torch.sigmoid(Wo @ concat + bo)\n",
    "\n",
    "\n",
    "    c = f*c_prev + u*cc\n",
    "    a = o*torch.tanh(c)\n",
    "#     y = Wy @ a + by\n",
    "#     return y, a, c\n",
    "    return a, c\n",
    "\n",
    "# test forward step\n",
    "parameters = initialize_lstm_parameters()\n",
    "batch_len=100\n",
    "a_prev = torch.randn((hidden_size,batch_len)).to(device)\n",
    "c_prev = torch.randn((hidden_size,batch_len)).to(device)\n",
    "x = torch.zeros((vocab_size, batch_len)).to(device)\n",
    "a, c = lstm_step_forward(x, a_prev, c_prev, parameters)\n",
    "a.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 10]), torch.Size([128, 10]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lstm_forward(batch_X, a_prev, c_prev, parameters):\n",
    "    Wf, bf, Wu, bu, Wcc, bcc, Wo, bo, Wy, by = parameters\n",
    "    batch_size = batch_X.shape[-1]\n",
    "\n",
    "    a = torch.zeros((sequence_len+1, hidden_size, batch_size)).to(device)\n",
    "    c = torch.zeros((sequence_len+1, hidden_size, batch_size)).to(device)\n",
    "    a[0] = a_prev\n",
    "    c[0] = c_prev\n",
    "   \n",
    "    for t in range(sequence_len):\n",
    "        a[t+1], c[t+1] = lstm_step_forward(batch_X[t], a[t].clone(), c[t].clone(), parameters)\n",
    "    y_hat = Wy @ a[t+1].clone() + by\n",
    "    \n",
    "    # loss = torch.sum(-y*torch.log(y_hat))\n",
    "    return y_hat, a[-1].detach(), c[-1].detach()\n",
    "\n",
    "# test forward operation\n",
    "parameters = initialize_lstm_parameters()\n",
    "m=10\n",
    "x,y = vectorization(X[:m],Y[:m])\n",
    "a_prev = torch.randn((hidden_size,m)).to(device)\n",
    "c_prev = torch.randn((hidden_size,m)).to(device)\n",
    "for i in range(10):\n",
    "    y_hat, a_prev, c_prev = lstm_forward(x, a_prev, c_prev, parameters)\n",
    "a_prev.shape, c_prev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viết câu đầu tiên, ít hơn 40 kí tự: cháu lên ba\n",
      "cháu lên ba ́ễåậ́ặỷ1́ặ1ẃwèấ;ậ́aấ;wè́ẽ'́ặỷỡwỷ́ễớ́ặ5ậ́wỷậưủ́ồớ́7ỷạ́wèỷð́7ỷủ.7́ặỷóõắ́ộớậ́ồớ́791wè́wèớṹ?ắậ́wỷấ́wèấ;ậ́ặỷ/ậ́7.̃́7ỷậð7”́791wè́795ậ́ồớ́aåậ́wèỷậð?́èậ̀̃́7ắ́ỷắậ́79ấ;wè́ặ)ắýẽớậ́7ỷ/́ồậð7́wắ?́79ẵ́ồ ́7ỷắ́7ỷậ●7́aấ*ặ́èậớủ́ặỷ8wè́hỷđwè́ặắ7ỷḿẽậmẃộậ ủ́ồớ́7ỷ●́wỷấwè́wèấ;ậ́aớẃẽớ́ồụẃhỷđwè́ặắễọwè́ễớ́ặỷậ●ặ́7ỷủũ ẃwè1ớậ́õắã́ặỷĩ́ồ.7́ẽĩ́ặỷủũðẃẽĩ́?ủ4ẃẽắṹặắ1́7ỷạ́ặỷỹwè́ặỷủũðẃ?i7́wèấ;ậ́?ợ́7ỷầṹwèấ;ậ́aớẃẽớ́ễåậ́ẽầ7́ỷåwỷ́hỷậ́ồ ́a●ẃ7ỷầṹaấ*ặ́wèấ;ậ́7ắ́aấ*ặ́ỷ1ớẃặẩwỷ́ồớ́ặ1ẃwèấ;ậ́7ỷấ;wè́ặỷủũðẃồớ́79ớwè́aḿhỷđwè́ặỷ\"́ặắ7ỷḿ7åậ́7ỷẩ1́ễớ́wèấ;ậ́aắwè́ặỷ1́ặỷ̀wè́p?́hỷđwè́75?́ỷậðẃ7ỷáặ́ếá́79ấ;wè́7ậwỷ́79ắwè́a“́èậ̀̃́7ắ́wỷầ7́79ẵ́7ỷớwỷ́ỷắậ́?ợ́ặ1ẃộâủ́ặ)ắ́7ạwỷ́ũưủ́aấ*ặ́hỷđwè́ặỷ\"́ặắwỷốwè́wèấ;ậ́aọặ́ẽậmẃhỷđwè́ặắ?i7́wèấ;ậ́aớẃẽớ́ỷớwè́wèớṹwỷâẃồ.7́79ấ;wè́79ưẃ791wè́7ỷ/́wè.̃́wỷậ ủ́wỷốwè́èậắ́aạwỷ́p?́ặỷủũðẃhḿồ ́wỷốwè́wèấ;ậ́7ỷắ́7ỷậ●7́ồqậ́?i7́ặđwè́ồậðặ́ặỷ1́ặỷ̀wè́7ắ́ặ̣wè́7ỷầṹ?i7́7ỷ●́7ỷạ́ễớ?́ẽåwỷ́7ỷâẃễâủ́ễớ́?i7́ặỷủũðẃẽú7́7ỷđậ́wèấ;ậ́ặỷ8wèã́?i7́wèớṹ7ỷĩ7́wè)́?ậưủ́7ẩ́ồ.ũã́ặắwè*ậ́ặỷậ●ẃaầủ́791wè́èậắ́aạwỷ́a●ẃồqậ́7ỷớwỷ́7ỷạ́wè1\n"
     ]
    }
   ],
   "source": [
    "def pickle_(file_path, data=None, operation='load'):\n",
    "    import bz2\n",
    "    import pickle\n",
    "    import _pickle as cPickle\n",
    "    \n",
    "    a = None\n",
    "    if operation != 'load':\n",
    "        with bz2.BZ2File(file_path, 'wb') as f:\n",
    "            cPickle.dump(data, f)\n",
    "    else:\n",
    "        with bz2.BZ2File(file_path, 'rb') as f:\n",
    "            a = cPickle.load(f)\n",
    "    return a\n",
    "\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64').reshape(-1)\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "   \n",
    "    out = np.random.choice(range(vocab_size), p = preds.ravel())\n",
    "    return out\n",
    "#     return np.argmax(preds)\n",
    "\n",
    "def softmax1d(x):\n",
    "    max, _ = torch.max(x, dim=0, keepdims=True) #returns max of each row and keeps same dims\n",
    "    e_x = torch.exp(x - max) #subtracts each row with its max value\n",
    "    sum = torch.sum(e_x, dim=0, keepdims=True) #returns sum of each row and keeps same dims\n",
    "    sf = e_x / sum \n",
    "\n",
    "    \n",
    "    return sf\n",
    "\n",
    "def generate_output(parameters, get_input=False):\n",
    "    '''\n",
    "    Generate n samples characters with random or set input\n",
    "    '''\n",
    "    generated = ''\n",
    "    m = len(X)\n",
    "    a_prev = torch.randn((hidden_size,1)).to(device)\n",
    "    c_prev = torch.randn((hidden_size,1)).to(device)\n",
    "    if get_input == False:\n",
    "        idx = int(np.random.choice(range(m),1))\n",
    "        sentence = [X[idx]]\n",
    "        Y_sample = [Y[idx]]\n",
    "    else:\n",
    "        usr_input = input(\"Viết câu đầu tiên, ít hơn 40 kí tự: \") + ' '\n",
    "        sentence = [('{0:0>' + str(sequence_len) + '}').format(usr_input[:-2]).lower()]\n",
    "        generated += usr_input\n",
    "        Y_sample = [usr_input[-1]]\n",
    "#         print(sentence, len(sentence[0]))\n",
    "    \n",
    "    for i in range(1000):\n",
    "        x , y = vectorization(sentence, Y_sample)\n",
    "        preds, a_prev, c_prev = lstm_forward(x, a_prev, c_prev, parameters)\n",
    "        preds = softmax1d(preds)\n",
    "        next_index = sample(preds.detach().cpu(), temperature = .5)\n",
    "            \n",
    "        next_char = ix_to_char[next_index]\n",
    "            \n",
    "        generated += next_char\n",
    "        sentence = [sentence[0][1:] + next_char]\n",
    "        \n",
    "    return generated\n",
    "\n",
    "# pickle_('params.pickle', parameters, 'write')\n",
    "parameters = pickle_('params.pickle')\n",
    "generated = generate_output(parameters, True)\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = len(X)\n",
    "batch_size = 1000\n",
    "num_batch = np.floor(m/batch_size)\n",
    "num_iterations = 20\n",
    "\n",
    "parameters = initialize_lstm_parameters()\n",
    "a_prev = torch.randn((hidden_size,batch_size)).to(device)\n",
    "c_prev = torch.randn((hidden_size,batch_size)).to(device)\n",
    "\n",
    "optimizer = optim.Adam(parameters, lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_iterations):\n",
    "    loss_total = 0\n",
    "    permutation = np.random.permutation(m)\n",
    "    count = 0\n",
    "    for j in range(0,m,batch_size):\n",
    "        indices = permutation[j:j+batch_size]\n",
    "        if len(indices) == batch_size: # skip last batch if m%batch_size!=0\n",
    "            batch_X, batch_Y = [X[i] for i in indices], [Y[i] for i in indices] # mini batch\n",
    "            batch_X, batch_Y = vectorization(batch_X, batch_Y)\n",
    "\n",
    "            y_hat, a_prev, c_prev =  lstm_forward(batch_X, a_prev, c_prev, parameters)\n",
    "            loss = criterion(y_hat.T, batch_Y)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_value_(parameters, 1)\n",
    "            optimizer.step()    # Does the update\n",
    "            optimizer.zero_grad()\n",
    "            count+=1\n",
    "            loss_total += loss.detach()\n",
    "\n",
    "    print('\\nIteration: %d, Loss: %f' % (i, loss_total/num_batch) + '\\n')\n",
    "    print(generate_output(parameters))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_('params.pickle', parameters, 'write')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
